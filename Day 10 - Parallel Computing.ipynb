{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Parallel Computing with cluster-helper\n",
    "\n",
    "<img src=\"https://computing.llnl.gov/tutorials/parallel_comp/images/nodesNetwork.gif\">\n",
    "\n",
    "A node is a like a computer within a much bigger computer!\n",
    "\n",
    "**Install ipython-cluster-helper**:\n",
    "\n",
    "* Clone repo from https://github.com/roryk/ipython-cluster-helper to your home directory\n",
    "* Activate your CML Anaconda environment (e.g. <code>source activate environmentname</code>)\n",
    "* Navigate to the ipython-cluster-helper directory and type <code>python setup.py install</code>\n",
    "\n",
    "(For some reason, this can take a while, so it may be advisable to have students do it at the end of the prior day's session)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cluster_helper.cluster\n",
    "\n",
    "def squared(x):\n",
    "    return x**2\n",
    "\n",
    "#You can essentially replicate this syntax for every time you use it:\n",
    "with cluster_helper.cluster.cluster_view(scheduler=\"sge\", queue=\"RAM.q\", num_jobs=10, cores_per_job=1) as view:\n",
    "    \n",
    "    #'map' applies a function to each value within an interable\n",
    "    res = view.map(squared, range(0, 10))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## cluster-helper tips\n",
    "\n",
    "* cluster-helper will act as if a fresh python notebook were started for each job, so it will not inherit your workspace's variables or import statements. **Give each job everything it needs to complete!**\n",
    "* Jobs are still subject to memory limitations, so you may need to **break up large processes into smaller chunks.** For example, each job could correspond to analyzing one session, instead of one subject. \n",
    "* The cluster-helper memory parameter does not work. Supposedly, this will be fixed eventually. If absolutely necessary, you may specify more cores per job to functionally increase the allotted memory. But mind your total core count!\n",
    "* It is often useful to save the output of each job in a dedicated directory, and sometimes useful to save intermediate values to aid in debugging or later nonparallel analyses. The Python \"os\" library can be helpful here. \n",
    "* **Be respectful!** There are only so many cores available to the entire Kahana lab and our collaborators across the country. \n",
    "* **Limit typical jobs to 100 cores or less**. Heavy usage means fewer resources for other users to use, and due to shared disk resources might actually slow down all jobs overall. Please ask for permission before using more.\n",
    "* You can always use the '**qdel**' command in Terminal, followed by your job number, to kill any of your old jobs that may be wasting rhino's resources. \n",
    "* Use the '**qstat**' command in Terminal to see cluster usage information.\n",
    "* Each rhino2 node has ~128 GB of memory and ~40 cores. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cluster Helper usage"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "See the separate ClusterHelper.ipynb file in this repository for example usage of cluster-helper.  As long as you follow the polite usage etiquette described above, you should feel free to customize this to your own needs.  It is helpful to follow the general principles shown there, such as saving computational results for each job directly to disk and logging exceptions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Skeleton of a parallel function\n",
    "subject_vars = []\n",
    "\n",
    "#Construct your function such that it takes one input variable which contains whatever info you need within it (i.e. a list or array)\n",
    "#Tip: Default arguments can often be an easy way to change the behavior of a parallel function. \n",
    "def tf_analysis(subject_vars, reref='bipolar'):\n",
    "    \n",
    "    #Sometimes useful to put entire function in a try...except block in case a subject/session breaks\n",
    "    try:\n",
    "    \n",
    "        sub = subject_vars[0]\n",
    "        exp = subject_vars[1]\n",
    "\n",
    "        ###YOUR CODE GOES HERE###\n",
    "\n",
    "        #Advisable to save outputs instead of relying on the outputs of view.map\n",
    "        np.save('myoutput.npy', output_data)\n",
    "    \n",
    "    except:\n",
    "        return\n",
    "    \n",
    "    return\n",
    "\n",
    "with cluster_helper.cluster.cluster_view(scheduler=\"sge\", queue=\"RAM.q\", num_jobs=10, cores_per_job=1) as view:\n",
    "    view.map(tf_analysis, subject_vars)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercise: Write a parallel function that returns the number of (bipolar) electrodes for every subject in the RAM dataset. Run with 5 jobs and 1 core per job.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Wrap Up/Q&A\n",
    "\n",
    "#### What did we learn over the past 2 weeks?\n",
    "\n",
    "* Loading experimental info & EEG\n",
    "* Spectral decomposition & time-frequency analysis\n",
    "* Statistics: T-tests, multiple comparisons, permutations\n",
    "* Phase-based functional connectivity\n",
    "* Machine learning: linear, logisitic regression, feature selection\n",
    "\n",
    "#### What did we NOT learn?\n",
    "\n",
    "* Cognitive modeling\n",
    "* Complex behavioral analyses\n",
    "* Single-unit activity\n",
    "* Spatial memory tasks\n",
    "* Brain stimulation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cml",
   "language": "python",
   "name": "cml"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
